{
  
    
        "post0": {
            "title": "DuckDuckGo Dataset Creation",
            "content": ". 1. Introduction . I near gave up on creating my own image scraped datasets when I tried using Bing&#39;s Image Search API. Me and Bing don&#39;t see eye-to-eye it seems. Good job DuckDuckGo swooped in to save the day. Below gives an overview of how I collated a couple of datasets to train some neural networks. The image scraper library I use is courtesy of Joe Dockerill. Big up to this guy for creating it. . 2. Image Scraping . import os from pathlib import Path import matplotlib.pyplot as plt from torchvision.datasets import ImageFolder from torch.utils.data import DataLoader from jmd_imagescraper.core import * from jmd_imagescraper.imagecleaner import * . The first dataset I want to create is a pets dataset. I&#39;d like to create a model that, given an image of a household pet, can state what type of pet it is. I&#39;m going with cats, dogs and rabbits to begin with. . # Path where I&#39;m going to save the scraped images path = Path().cwd()/&quot;pets&quot; # The types of pet I&#39;m interested in pets = [&quot;dog&quot;, &quot;cat&quot;, &quot;rabbit&quot;] # Scraping the images for pet in pets: duckduckgo_search(path, pet, pet, max_results=10, img_layout=ImgLayout.All); . The below function is an image cleaner widget that comes with Joe&#39;s library. Widgets are a great, user-friendly way of viewing, organising and preparing a dataset in preparation for training a model with it. . display_image_cleaner(path); . The above widget is also great for finding those pesky images that just shouldn&#39;t be there. For example, as much as I enjoyed finding this image in the &#39;dog&#39; folder, it doesn&#39;t quite fit my requirement. . . 3. Labelling the data . Quite often the labels of vision datasets are in the file name. I want to keep this system so in the below code I rename all of the files on my local machine to include the pet type. . n=0 while n &lt; len(pets): root = path/pets[n] files = os.listdir(root) label = str(pets[n]) n += 1 for index, file in enumerate(files): os.rename(os.path.join(root, file), os.path.join(root, &#39;&#39;.join([label, &#39;_&#39;, str(index), &#39;.jpg&#39;]))) . . Let&#39;s check this has done the job: . os.listdir(path/&quot;dog&quot;) . [&#39;dog_9.jpg&#39;, &#39;dog_8.jpg&#39;, &#39;dog_6.jpg&#39;, &#39;dog_7.jpg&#39;, &#39;dog_5.jpg&#39;, &#39;dog_4.jpg&#39;, &#39;dog_0.jpg&#39;, &#39;dog_1.jpg&#39;, &#39;dog_3.jpg&#39;, &#39;dog_2.jpg&#39;] . I&#39;m going to use the ImageFolder class in torchvision to load the pets dataset. . pets_dataset = ImageFolder(path) random_image = pets_dataset[14][0] random_image_label = pets_dataset[14][1] # Looking up the random_image_label in the pets_dataset dictionary label = [key for key, value in pets_dataset.class_to_idx.items() if value == random_image_label] # Print the Image using Matplotlib plt.imshow(random_image) plt.axis(&#39;off&#39;) print(&quot;The label of the image is:&quot;, label[0]) . The label of the image is: dog . 4. Tips &amp; Tricks . I tend to write a little section of tips and tricks that I&#39;ve learnt whilst writing a notebook. Below are the things that looked into this time. They revolve around environments, how to create them and how to view the packakges installed on them. I was looking into this as I ideally want to create an environment that I know all of these notebook can be run on and avoid breaking any package version dependencies. . To determine the Anaconda environments available, run the line below. The current environment is the one in the list that is highlighted with an asterisk (*). . conda info --envs . To see a list of all packages installed in a specific environment (here I&#39;ve chosen &quot;ctenv&quot;), run: . conda list -n base . To see if a specific package (here I&#39;ve chosen fastai) is installed in an environment, run: . conda list -n base fastai . To install a specific package (here I&#39;ve chosen seaborn) into an environment, run: . conda install -n base seaborn .",
            "url": "https://mathschelsea.github.io/Blogging/jupyter/2021/12/07/imagescraping.html",
            "relUrl": "/jupyter/2021/12/07/imagescraping.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Under the hood of a Neural Network",
            "content": "1. An Old Friend . 1.0 Neural Network Definition . The definition of a simple (3-layer) neural network is as follows: . def simple_nn(x): linear1 = (x * w1) + b1 ReLU = linear1.max(tensor(0.0)) linear2 = (ReLU * w2) + b2 return linear2 . Now for those not familiar with Python code, or any code, this can look a little daunting. But bear with me here as it may not seem as bad after breaking it down. Let&#39;s go through each line. . 1.1 Breaking it down . def simple_nn(x): is stating that we&#39;re about to define a function simple_nn based on one variable x . linear1 = (x * w1) + b1 is defining a new variable linear1 which is a straight line equation involving variable x and the parameters w1 (weight) and b1 (bias). . ReLU = linear1.max(tensor(0.0)) is defining another new variable ReLU which is take the maximum value of either linear1 or 0 i.e. it&#39;s a function that looks like this: . import torch from torch import tensor import matplotlib.pyplot as plt t = torch.arange(-2.0,2,0.5).float() yt = t.max(tensor(0.)) plt.plot(t, yt); . . linear2 = (ReLU * w2) + b2 is defining a final variable linear2 which is a straight line equation involving variable ReLU and the parameters w2 (weight) and b2 (bias). . return linear2 does what it says on the tin, it returns the value of linear2. . So, what does this mean? It means that the definition of a simple neural net is based equations such a that of a straight line . y = mx + c . and the max function: . f(x) = max(x,0) . which are both equations that we were most likely taught at the ages of 15. . Does it really work? . Some might say that such a simple set-up can&#39;t possible solve problems such as recognising dogs in images or distinuishing voices in a crowd. They&#39;d be right in the sense that the above (very) simple definiton can&#39;t but if more layers (linear equations and max functions) were added then; yes, it can. In fact, it can solve any computable problem to an high level of accuracy with the right weights and biases. This is due to it&#39;s interesting setup of linear (linear equation) and nonlinear (max function) layers. . But there&#39;s no need to take my word for it. We can see if it works... . 2. MNIST Data . 2.0 Importing the data . Pytorch has a number of very nice datasets that you can use for modelling. The one we&#39;ll use today to test our simple neural network is the MNIST dataset. It is a dataset of handwritten digits. . from pathlib import Path from torchvision import datasets path = Path().cwd()/&quot;Data&quot; ds_train = datasets.MNIST(path, train=True, download=True) ds_valid = datasets.MNIST(path, train=False, download=True); figure = plt.figure(figsize=(10,8)) r, c = 3, 3 for i in range(1,r*c+1): img, label = ds_train.data[i], ds_train.targets[i].item() figure.add_subplot(r, c, i) plt.title(label) plt.imshow(img, cmap=&quot;gray_r&quot;) plt.axis(&#39;off&#39;) plt.show() . . 2.1 The Matrix . Now in true Matrix fashion, everything we see is made up of numbers. That is, each pixel has a number associated to it that is translated into a grayscale colour to make up the images we see above. To see the image in it&#39;s natural numerical form we just view it as a matrix. The images are 28 x 28 pixels so the matrix has 28 rows and 28 columns. . import pandas as pd im3 = ds_train.data[7] df = pd.DataFrame(im3).astype(int) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 38 | 43 | 105 | 255 | 253 | 253 | 253 | 253 | 253 | 174 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 139 | 224 | 226 | 252 | 253 | 252 | 252 | 252 | 252 | 252 | 252 | 158 | 14 | 0 | 0 | 0 | 0 | 0 | . 7 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 178 | 252 | 252 | 252 | 252 | 253 | 252 | 252 | 252 | 252 | 252 | 252 | 252 | 59 | 0 | 0 | 0 | 0 | 0 | . 8 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 109 | 252 | 252 | 230 | 132 | 133 | 132 | 132 | 189 | 252 | 252 | 252 | 252 | 59 | 0 | 0 | 0 | 0 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 29 | 29 | 24 | 0 | 0 | 0 | 0 | 14 | 226 | 252 | 252 | 172 | 7 | 0 | 0 | 0 | 0 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 85 | 243 | 252 | 252 | 144 | 0 | 0 | 0 | 0 | 0 | 0 | . 11 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 88 | 189 | 252 | 252 | 252 | 14 | 0 | 0 | 0 | 0 | 0 | 0 | . 12 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 91 | 212 | 247 | 252 | 252 | 252 | 204 | 9 | 0 | 0 | 0 | 0 | 0 | 0 | . 13 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 32 | 125 | 193 | 193 | 193 | 253 | 252 | 252 | 252 | 238 | 102 | 28 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 14 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 45 | 222 | 252 | 252 | 252 | 252 | 253 | 252 | 252 | 252 | 177 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 15 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 45 | 223 | 253 | 253 | 253 | 253 | 255 | 253 | 253 | 253 | 253 | 74 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 16 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 31 | 123 | 52 | 44 | 44 | 44 | 44 | 143 | 252 | 252 | 74 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 17 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 15 | 252 | 252 | 74 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 18 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 86 | 252 | 252 | 74 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 19 0 | 0 | 0 | 0 | 0 | 0 | 5 | 75 | 9 | 0 | 0 | 0 | 0 | 0 | 0 | 98 | 242 | 252 | 252 | 74 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 20 0 | 0 | 0 | 0 | 0 | 61 | 183 | 252 | 29 | 0 | 0 | 0 | 0 | 18 | 92 | 239 | 252 | 252 | 243 | 65 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 21 0 | 0 | 0 | 0 | 0 | 208 | 252 | 252 | 147 | 134 | 134 | 134 | 134 | 203 | 253 | 252 | 252 | 188 | 83 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 22 0 | 0 | 0 | 0 | 0 | 208 | 252 | 252 | 252 | 252 | 252 | 252 | 252 | 252 | 253 | 230 | 153 | 8 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 23 0 | 0 | 0 | 0 | 0 | 49 | 157 | 252 | 252 | 252 | 252 | 252 | 217 | 207 | 146 | 45 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 24 0 | 0 | 0 | 0 | 0 | 0 | 7 | 103 | 235 | 252 | 172 | 103 | 24 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 25 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 26 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 27 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2.2 Defining x . So now we have our images, how do we feed them into our simple neural network in order to train it to distinguish between handwritten digits? . 3. Setting up the data . 3.0 Transforming the images . As discussed, our handwritten digits can be viewed as images and as 28x28 matrices. They can also viewed as a vector i.e. lining the rows of the 28 x 28 matrix up one after the other to make a vector with 1 row and 784 columns. . Here is our image three again: . figure = plt.figure(figsize=(3,3)) plt.imshow(im3, cmap=&quot;gray_r&quot;) plt.axis(&#39;off&#39;); . . And here is it&#39;s shape (a 28 x 28 matrix): . print(im3.shape) . torch.Size([28, 28]) . Here is our image three after it&#39;s been manipulated into a vector: . vector3 = ds_train.data[3].view(-1,28*28) plt.rcParams[&quot;figure.figsize&quot;] = (10,10) plt.imshow(vector3, cmap=&quot;gray&quot;) plt.axis(&#39;off&#39;); . . And here is it&#39;s shape (note 28 x 28 = 784): . print(vector3.shape) . torch.Size([1, 784]) . 3.1 Inputs for the NN . We&#39;re at a stage now where we can start defining some of the variables in our simple neural network: . def simple_nn(x): linear1 = (x * w1) + b1 ReLU = linear1.max(tensor(0.0)) linear2 = (ReLU * w2) + b2 return linear2 . The x variable comprises of our images as vectors. For simplicity we&#39;ll just use 4 images of ones and 4 images of threes: . list_1_3 = [3, 6, 7, 8, 10, 12, 14, 27] n=1 for i in list_1_3: plt.subplot(1,len(list_1_3),n) plt.imshow(ds_train.data[i], cmap=&quot;gray_r&quot;) plt.axis(&#39;off&#39;) n = n + 1 plt.show() . . So x is made up of 8 vectors representing images of ones and threes. This means x is a 8 x 784 matrix. Let&#39;s just check this: . x = ds_train.data[list_1_3]/255 x = x.view(-1, 28*28) print(&quot;x shape check:&quot;,x.shape) . . x shape check: torch.Size([8, 784]) . The weights, w1 and bias b1 are parameters that we don&#39;t know yet therefore we can set them to arbitary random numbers. The idea is that as we train our neural network, we&#39;ll update these weights and biases so that the loss function reduces and thus improves the accuracy of the model. . Referring back to our training loop. The x variable is the input (i.e the images are the inputs). The parameters are our weights, w1 and bias b1 and the architecture is our simple neural network. . 3.2 First run through . We&#39;ve already defined our x variable as the 8 x 784 matrix. Let&#39;s now initalise a random set of parameters: . w1 = torch.randn((784,1)) b1 = torch.randn(1) w1.shape, b1.shape . (torch.Size([784, 1]), torch.Size([1])) . Putting this all together, we get our linear1 function: . def linear1(x): return x@w1 + b1 linear1(x) . tensor([[ -7.4666], [ -6.3561], [-17.4898], [ -4.0711], [-19.8116], [ -4.9169], [ -4.3286], [-15.3125]]) . What the linear1 function has done is it&#39;s taken every pixel in an image and multiplied it&#39;s value by a weight and then added a bias. If the linear1 function&#39;s aim is to predict whether the image shows a number three, then the weights would ideally penalise pixels that follow a straight line pattern and reward pixels that take on curves, as an example. . We can check how well this first linear equation is at predicting whether the handwritten digit on the image is a three by defining an accuracy function and comparing the linear1 outputs to the actuals. . # Defining the actuals _y = ds_train.targets[list_1_3].unsqueeze(1) y = _y == 3 y = y.float() . . Here are the actuals: . print(y) . tensor([[0.], [0.], [1.], [0.], [1.], [1.], [0.], [1.]]) . Here is our accuracy function: . def accuracy(predictions, actuals): corrects = (predictions &gt; 0.0).float() == actuals return corrects.float().mean().item() . Now let&#39;s see how well our linear1 function is at distinguishing images of handwritten threes vs. handwritten ones: . predictions = linear1(x) actuals = y accuracy(predictions, actuals) . 0.5 . A reasonable first attempt for a random set of weights and bias. The idea now is to improve on the parameters w1 and b1 so that this accuracy improves. The way this is done by defining a loss function and using Stochastic Gradient Descent (SGD). The loss function measures how well the model performs based on the current weights and biases. SGD is a type of optimisation that aims to tweeks the weights and biases so as to minimise the loss function. . The introduction of the ReLU function and the linear2 equations will improve the accuracy of the neural network even further. It will able to hone in on more features in the images and detect objects of increasing complexity, such as dogs. . 4. Training a Neural Network . 4.0 Training and Validation datasets . # Training data # Pull out the 3s filter_3 = ds_train.targets == 3 x_train_3, y_train_3 = ds_train.data[filter_3]/255, ds_train.targets[filter_3] # Pull out the 7s filter_7 = ds_train.targets == 7 x_train_7, y_train_7 = ds_train.data[filter_7]/255, ds_train.targets[filter_7] # Concatenate the 3s and 7s and change them from a list of matrices to a list of vectors x_train = torch.cat([x_train_3, x_train_7]).view(-1, 28*28) # Concatenate the labels and use &#39;1&#39; for 3s and 0 for 7s. y_train = torch.tensor([1]*len(y_train_3)+[0]*len(y_train_7)).unsqueeze(1) # Validation data # Pull out the 3s filter_3 = ds_valid.targets == 3 x_valid_3, y_valid_3 = ds_valid.data[filter_3]/255, ds_valid.targets[filter_3] # Pull out the 7s filter_7 = ds_valid.targets == 7 x_valid_7, y_valid_7 = ds_valid.data[filter_7]/255, ds_valid.targets[filter_7] # Concatenate the 3s and 7s and change them from a list of matrices to a list of vectors x_valid = torch.cat([x_valid_3, x_valid_7]).view(-1, 28*28) # Concatenate the labels and use &#39;1&#39; for 3s and 0 for 7s. y_valid = torch.tensor([1]*len(y_valid_3)+[0]*len(y_valid_7)).unsqueeze(1) . . x_train.shape, y_train.shape . (torch.Size([12396, 784]), torch.Size([12396, 1])) . x_valid.shape, y_valid.shape . (torch.Size([2038, 784]), torch.Size([2038, 1])) . from torch.utils.data import DataLoader training = zip(x_train, y_train) training = list(training) training = list(zip(x_train, y_train)) validation = list(zip(x_valid, y_valid)) dl_train = DataLoader(training, batch_size=256, shuffle=True) dl_valid = DataLoader(validation, batch_size=256, shuffle=True) . xb1, yb1 = next(iter(dl_train)) xb1.shape, yb1.shape . (torch.Size([256, 784]), torch.Size([256, 1])) . 4.1 Initialisation . w1 = torch.randn((784,30)).requires_grad_() w2 = torch.randn((30,1)).requires_grad_() # Biases b1 = torch.randn(30).requires_grad_() b2 = torch.randn(1).requires_grad_() # Parameters parameters = w1, w2, b1, b2 # Learning Rate lr = 1.0 # Simple Neural Network def simple_net(xb): linear1 = xb@w1 + b1 ReLU = linear1.max(tensor(0.0)) linear2 = ReLU@w2 + b2 return linear2 . 4.2 Loss Function . def sigmoid(x): return 1/(1+torch.exp(-x)) # Loss function def mnist_loss(predictions, targets): predictions = predictions.sigmoid() return torch.where(targets==1, 1-predictions, predictions).mean() . 4.3 Gradient Calculator . def calc_grad(xb, yb, model): predictions = simple_net(xb) loss = mnist_loss(predictions, yb) loss.backward() . 4.4 Accuracy Function . def batch_accuracy(xb, yb): predictions = xb.sigmoid() correct = (predictions &gt; 0.5) == yb return correct.float().mean() . 4.5 Training &amp; Validation functions . def train_epoch(model, lr, parameters): for xb, yb in dl_train: calc_grad(xb, yb, model) for p in parameters: p.data -= p.grad * lr p.grad.zero_() . def validate_epoch(model): accuracys = [batch_accuracy(model(xb), yb) for xb, yb in dl_valid] return round(torch.stack(accuracys).mean().item(), 4) . 4.6 Training &amp; Validating the model . for i in range(10): train_epoch(simple_net, lr, parameters) print(validate_epoch(simple_net), end=&#39; &#39;) . 0.9264 0.9568 0.9632 0.97 0.9691 0.975 0.9749 0.9749 0.976 0.977 . 5. Testing a Neural Network . 5.0 Importing my handwritten images . Normally we&#39;d access the true accuracy of a model by pushing a test dataset through it. I still want to do this but I thought I&#39;d have some fun and test the model&#39;s accuracy using my own handwritten images. In this section, I&#39;ll load some of my images and display them. . from matplotlib import image x_test = [] for i in range(6): test_im = image.imread(&quot;images/2021-12-05-underthehood&quot;+str(i)+&quot;.jpg&quot;) x_test.append(test_im) n=1 for i in range(6): plt.subplot(1,len(range(6)),n) plt.imshow(x_test[i][:,:,1], cmap=&quot;gray_r&quot;) plt.axis(&#39;off&#39;) n = n + 1 plt.show() . . Let&#39;s see how well our simple Neural Network classes these handwritten figures: . figure = plt.figure(figsize=(10,8)) r, c = 2, 3 for i in range(r*c): test_array = x_test[i][:,:,1] test_tensor = tensor(test_array) test = test_tensor.view(-1, 28*28)/255 pred = simple_net(test).sigmoid() correct = pred.item() == 1 figure.add_subplot(r, c, i+1) plt.title(&quot;This image is a 3 : &quot;+str(correct)+&quot; n Probability a 3 : &quot;+&quot;{:.0%}&quot;.format(pred.item())) plt.imshow(test_array, cmap=&quot;gray&quot;) plt.axis(&#39;off&#39;) plt.show(); . .",
            "url": "https://mathschelsea.github.io/Blogging/jupyter/2021/12/05/underthehood.html",
            "relUrl": "/jupyter/2021/12/05/underthehood.html",
            "date": " • Dec 5, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Cats vs. Dogs",
            "content": ". 1. Introduction . Below shows the workings and investigations that have come about from doing the first lesson of the Fastai Deep Learning for Coders MOOC. As stated in my first blog post, this blog doesn&#39;t aim to impress, it&#39;s merely a documentation of a beginners journey into deep learning which may; or may not, help others in the future. . 2. Cats &amp; Dogs Vision Tutorial . 2.1 Vision Tutorial - from_name_func . I started the first Fastai Deep Learning for Coders lesson about a week before I actually ran the first box of code, error-free. This was due &#39;ImageDataLoaders&#39; not working which boiled down to my pillow package version not being compatible with other packages (I downgraded to pillow 8.2.0). Whilst I was losing hope with my pillow issues I came across the &#39;Tutorials&#39; section on the Fastai Docs site. I had been told about these in the past but forgot about them in my quest to get my AWS notebook instance working (link here on how I did this). Looking through the tutorials I decided that I would use them in conjunction with the lessons as they seemed pretty decent. In particular, I like the more incremental breakdown of the code. Therefore, I will start this first lesson with going through the Vision tutorial and playing around with a few things. . from fastai.vision.all import * path = untar_data(URLs.PETS) # Let&#39;s see what is inside this path print(&quot;Path:&quot;, path) print(&quot;Path contents:&quot;, path.ls()) . Path: /home/ec2-user/.fastai/data/oxford-iiit-pet Path contents: [Path(&#39;/home/ec2-user/.fastai/data/oxford-iiit-pet/images&#39;), Path(&#39;/home/ec2-user/.fastai/data/oxford-iiit-pet/annotations&#39;)] . I&#39;m only interested in the &#39;images&#39; folder for now. I&#39;ll use the get_image_files function to grab all of the image files (recursively) from the folder and put them in the &#39;files&#39; variable. . files = get_image_files(path/&#39;images&#39;) print(&quot;Number of objects in &#39;files&#39; variable:&quot;,len(files)) print(&quot;First file:&quot;,files[0]) print(&quot;Third file:&quot;,files[2]) . Number of objects in &#39;files&#39; variable: 7390 First file: /home/ec2-user/.fastai/data/oxford-iiit-pet/images/wheaten_terrier_96.jpg Third file: /home/ec2-user/.fastai/data/oxford-iiit-pet/images/Russian_Blue_160.jpg . There are 7,390 objects in the &#39;files&#39; variable. For the minute they are locations of files. In order to view the actual images we must use the PILimage.create function. . import matplotlib.pyplot as plt n = [0,1,2,3,4] r = 1 # Number of rows c = 5 # Number of columns s = 1 # Subplot counter fig = plt.figure(figsize=(15,15)) for i in n: plt.subplot(r,c,s) plt.imshow(PILImage.create(files[i])) plt.axis(&#39;off&#39;) s = s+1 . In order to build a supervised model from these images we need labels. The file names are labels, they state what bread the animal is in the image. The file names also state whether the image is of a dog or cat based on the first letter in the file name. If the first letter of the file name is a capital then that image is of a cat. Let&#39;s define a label function based on the first letter of the image name: . def label_func(f): return f[0].isupper() im = PILImage.create(files[0]) im.show(figsize=(4,4)) print(&quot;Below is an image of a cat, T/F?:&quot;,label_func(files[0].name)) . Below is an image of a cat, T/F?: False . In order to get the images ready for a model, they need to be put into a DataLoaders object. . dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(224)) . The following is happening in the above section of code: . Telling the function the directory I&#39;m working in; | The files that I&#39;ve grabbed from that directory; | The function that defines the labels; and | Stated the transformation I want to apply (resize all images to 224x224 pixels). | Note that the function &#39;from_name_func&#39; is just an extention of the &#39;from_path_func&#39;. I&#39;ll show this later on. Also note that there are a number of additional arguments. From the documents: . ImageDataLoaders.from_name_func(path, fnames, label_func, valid_pct=0.2, seed=None, item_tfms=None, batch_tfms=None, bs=64, val_bs=None, shuffle=True, device=None) . The data is randomly split into training and validation sets based on the valid_pct input (with the option to seed the outcome). Any error or performance metrics will be calculated on the validation dataset. The validation data is a random subset of valid_pct based on val_bs which is the validation batch size. If val_bs isn&#39;t provided then bs is used by default. Similarly, the training data is a random subset based on bs which defaults to 64. . print(&quot;Training data batch size:&quot;, dls.train.bs) print(&quot;Validation data batch size:&quot;, dls.train.bs) print(&quot;Dependent variable unique level names:&quot;, dls.vocab) print(&quot;Dependent variable number of unique level names:&quot;, dls.c) . . Training data batch size: 64 Validation data batch size: 64 Dependent variable unique level names: [False, True] Dependent variable number of unique level names: 2 . dls.train.show_batch() . 2.2 Vision Tutorial - from_path_func . As I mentioned above, I could have used a different ImageDataLoaders function to create a DataLoaders object containing the pet images. Here we&#39;ll use the ImageDataLoader.from_path_func and define a new label function. . def label_func2(f): return f.name[0].isupper() im = PILImage.create(files[100]) im.show(figsize=(4,4)) print(&quot;Below is an image of a cat, T/F?:&quot;,label_func2(files[100])) . Below is an image of a cat, T/F?: True . dls2 = ImageDataLoaders.from_path_func(path, files, label_func2, item_tfms=Resize(224)) dls2.show_batch() . 2.3 Vision Tutorial - DataBlocks . Datablocks can be used to gather the data before it is passed to a dataloader. You can think of datablocks as a blue print or recipe that tells you how to assemble the data it will everytually be passed. This setup is a little less user friendly but I wanted to try get to grisps with it in case I want to use it in the future. . def label_func3(f): return &quot;cat&quot; if f.name[0].isupper() else &quot;dog&quot; im = PILImage.create(files[373]) im.show(figsize=(4,4)) print(&quot;Below is an image of a&quot;,label_func3(files[373])) . Below is an image of a dog . pets = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(), get_y=label_func3, item_tfms=Resize(224)) . ImageBlock states what data type the independent variables are (images in this case). | CategoryBlock states what data type the dependent variable is (categorical variable in this case). | get_items provides the data (a folder of images file names in this case). | splitter defines how the validation set is created. | get_y defines the target variable that should be used (the label function) | item_tfms defines how each image is transformed (224 x 224 pixels). | . dls3 = pets.dataloaders(path/&quot;images&quot;) dls3.show_batch() . 3. Fastai Lesson One . 3.1 The first model . The first model is run roughly half way into the first lesson. Everything spoken about prior to that point is extremely useful and insightful but it is at this halfway point that I felt that I needed my AWS notebook instance up and running. Thus I&#39;m going to start from that stage in this script. . def is_cat(x): return x[0].isupper() dls = ImageDataLoaders.from_name_func(path, get_image_files(path), valid_pct=0.2, seed=42, label_func=is_cat, item_tfms=Resize(224)) learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(3) . epoch train_loss valid_loss error_rate time 0 0.376693 0.240782 0.112991 01:05 epoch train_loss valid_loss error_rate time 0 0.246106 0.195579 0.078484 01:32 1 0.190142 0.171598 0.070027 01:33 2 0.144727 0.148527 0.058863 01:35 . Note: one epoch = one instance of the model looking through every image in the dataset once. . From the table above, I can see that as the model runs through more epochs the error rate (the proportion of images that were incorrectly identified) reduces. After running through the pets datasets 3 times, this model can tell the difference between an image of a dog and cat with great accuracy after less than 10 minutes run time (and very little knowledge!). This level of performance is not a given, resenet34 is a competition-winning model that has been pretrained on over a million images. Thus, fine-tuning this model on a new, more refined set of images (the act of which is called transfer learning) means I had a great starting point. . 3.2 Testing the model . I&#39;m going to test this model to see if it can correctly identify that my dog, Atlas, is in fact a dog (it is were a dog vs. cloud competition I may not be so optimistic). Here is the image of Atlas that I will use: . . from ipywidgets import FileUpload uploader = FileUpload() uploader . img = PILImage.create(uploader.data[0]) is_cat,_,probs = learn.predict(img) print(f&quot;Is this a cat?: {is_cat}.&quot;) print(f&quot;Probability it&#39;s a cat: {probs[1].item():.6f}&quot;) . Is this a cat?: False. Probability it&#39;s a cat: 0.000251 . Well, well, well. He&#39;s not a cat. And the model is pretty certain he isn&#39;t. I&#39;m impressed but at the same time scared by the vastness of what I don&#39;t know! . 4. Tips &amp; Tricks . Below is a list of smaller things I learnt over the course of lesson one that I found useful and dont want to forget. . Pressing &#39;H&#39; in command mode brings up a list of all functions available in command mode. | Using &#39;#&#39; in a code cell tells Python that what follows is a comment and so it should be ignored. | The kernel for this notebook is shown in the top right hand conrner under the &#39;Logout&#39; button. | An example of how to check a package&#39;s version and location is given below: | import PIL print(PIL.__version__) print(PIL.__file__) . The &#39;-qqq&#39; after an install command keeps a notebook clean. The &#39;q&#39; essentially stand for &#39;quiet&#39;. A single &#39;q&#39; will only show warnings and errors. Two &#39;q&#39;s only shows errors and three &#39;q&#39;s will disable all outputs. | I&#39;ve used quite a variety of code to show the images in this notebook. One I haven&#39;t used but want to remember is the following: | img = PILImage.create(files[1234]) img.to_thumb(192) .",
            "url": "https://mathschelsea.github.io/Blogging/jupyter/2021/07/28/lessonone.html",
            "relUrl": "/jupyter/2021/07/28/lessonone.html",
            "date": " • Jul 28, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "AWS NVIDIA GPU Instance Setup (for UK)",
            "content": ". 1. Prelimiary Points . 1.1 Why do I need an NVIDIA GPU? . I&#39;m completeing the Deep Learning for Coders with fastai &amp; PyTorch course by Jeremy Howard and Sylvain Gugger which; by the way, is amazing and very alturistic of them (thanks both!). To actually do the course I need access to a GPU. Now, I&#39;m in no way an expert on software engineering but through reading the course book it says that the tasks handled by a GPU are very similar to those done by a neural network. Therefore, for this course, computing over a GPU is superior to that of a CPU. In addition, not any old GPU will be supported by the main deep learning libraries therefore I need access to an NVIDIA GPU. . 1.2 Why am I using AWS? . I&#39;ve decided to complete this course using Amazon Sagemaker notebook instances. The reason for this is because I already use AWS at work but mainly as a cloud storage platform (s3 buckets). I&#39;d like to see what other wizardry it houses. . 1.3 Alternative Guides . There are a couple of guides on how to get set-up with Amazon Sagemaker as well as other servers such as Paperspace Gradient and Google Colab. However, I&#39;ve found that some of the steps are different for people living in the UK so I thought I&#39;d create this slightly alternative guide. I&#39;m also very new to remote server usage and deep learing so I thought I&#39;d create a more incremental guide for the absolute beginners among us (I feel your pain). . 2. AWS Account &amp; Service Quota Increase . 2.1 AWS Account . In order to use Amazon Sagemarker you need an AWS account. This is really simple to set-up, just follow Amazon&#39;s instructions in this post here. Note, you&#39;ll need your credit or debit card details to hand. . 2.2 Service Quota Increase . When you create an AWS account you&#39;ll be assigned the default quotas (also referred to as &#39;limits&#39;) for a variety of services (images, instances etc.) based on your region (I&#39;m based in the UK so my region is eu-west-2). These defaults might work for you or you may want to go up a notch with them. As I said at the start, for the Fastai course I need a specific GPU and so I need to request a &#39;limit increase&#39;. . Now as I&#39;ve said, I&#39;m no software engineer and I had a bit of a hard time following the quides at this point. According to the steps, I needed to request a &#39;ml.p2.xlarge&#39; instance however this isn&#39;t currently available in my region. I then went in search of the next best thing and found this wonderful article which helped me decide on requesting a &#39;g4dn.xlarge&#39; instance. . Note, the request can take a day or two for AWS support to process. So make sure you account for this if you&#39;re trying to stick to a schedule or deadline. . 2.3 Steps for request . 1 - On the AWS console home page find the &#39;Support&#39; drop down located in the top right hand corner and select &#39;Support Center&#39; from the list. . . 2 - On the support center home page select &#39;Create Case&#39;. . . 3 - Then select &#39;Service Limit Increase&#39;. This will populate the lower part of the page. . . 4 - In &#39;Limit type&#39; select &#39;SageMaker Notebook Instances&#39;. . . 5 - Select the region (I chose &#39;EU (London)&#39;) and the Limit (I selected &#39;ml.g4dn.xlarge&#39;). . . 6 - Add a case description (I said I was completed thing Fastai course and needed an NVIDIA GPU at an affordable price). . 7 - Press &#39;Submit&#39;. . 8 - A new case will be opened in the &#39;Support Center&#39; and an AWS support will be in touch with you. . 3. CloudFormation . 3.1 CloudFormation . Once your service quota increase has been approved it&#39;s time to create a CloudFormation stack. I think of these stacks as your blueprint. It&#39;s a set of resources that you define and freeze at a point in time. It means that you can replicate a certain situation or environment again and again. Frequently when using Python or Conda packages there will be an update to one package which is incompatbile with another. This can result in a script being fine one day but failing the next day after a package update. It&#39;s really annoying. Having an environment that is frozen in time means that your code can run without fail well into the future. I&#39;m sure the advantage of using stacks isn&#39;t limited to just this but it&#39;s what stood out to me. . 3.2 Stack Templates . A series of stack templates are provided on the Fastai site however; as I&#39;ve stated before, these are set-up with an instance type of &#39;ml.p2.xlarge&#39; which isn&#39;t available in the EU London (eu-west-2) region. Therefore, what I did was download the YAML file and edit it so that the instance choices include &#39;ml.g4dn.xlarge. I then saved this new YAML file to an S3 bucket of my own and created a stack based on that. . 3.3 Stack Setup . 1 - Download the Fastai EU London template using this link. . 2 - Edit the default instance type to be &#39;ml.g4dn.xlarge&#39; (line 4 below) and also add it as an allowed values (line 8). I did this edit in Visual Studio Code for ease and saved the edits over the original YAML file. . . 3 - Steps 3 to 6 guide you through uploading your YAML file to an S3 bucket so that your CloudFormation stack can access it as a stack template. This isn&#39;t necessary therefore if you&#39;re not interested in S3 buckets or knowing this bit then skip to step 7. Next thing to do is to upload this edited YAML file to an S3 bucket. To do this, return to your AWS management console page and in the search bar type &#39;S3&#39; and select &#39;S3&#39; from the drop down list. . . 4 - In order to upload the YAML file to an S3 bucket, you need to create one. Just click the &#39;Create Bucket&#39; button, give your bucket a name (I called mine &#39;ct-fastai-coursev4&#39;) and scrole down to &#39;Create Bucket&#39;. You should see your S3 bucket in the list like mine below: . . 5 - Now, click on this new bucket and upload your YAML file. You&#39;ll do this by selecting &#39;Upload&#39; and doing some drag and drop action with your YAML file. Once done your file should appear in the bucket. . . 6 - Click on your YAML file that you&#39;ve just uploaded and copy the objects URL. You&#39;ll need this when creating your CloudFoundation stack. . 7 - Time to go back to the AWS management console page and in the search bar type &#39;CloudFormation&#39; and select &#39;CloudFormation&#39; from the drop down list. . . 8 - Click the &#39;Create Stack&#39; button. On the form select &#39;Template is ready&#39;. If you&#39;re coming straight from step 3 then you should select your template source as &#39;Upload a template&#39; and chose your edited YAML file from step 2. If you&#39;ve gone through steps 3 to 6 then you can select your template source as &#39;Amazon S3 URL&#39; and paste in the object URL from step 6. . . 9 - Enter a stack name (I called it FastaiSageMakerStack) , the instance type as &#39;ml.g4dn.xlarge&#39; and volume size as &#39;50&#39; and click &#39;Next&#39;. I then clicked &#39;Next&#39; for the following page without changing anything and created the stack. The below page should then appear which shows the stack being created. . . 10 - Wait for the stack&#39;s status to change to &#39;CREATE_COMPLETE&#39; before moving on. . . 4. Sagemaker Instance . 4.1 Notebook Setup . 1 - Go to the AWS Management Console page and type &#39;Sagemarker&#39; in the search bar. Select &#39;Amazon SageMaker&#39;. . . 2 - On the right hand panel select &#39;Notebook&#39; and then &#39;Notebook Instances&#39;. A page should appear with your notebook instances, in the list should be the notebook &#39;fastai-v4&#39; that was created from our CloudFormation stack (line 196 of the YAML file). . . 3 - Right click on the &#39;Open in Jupyter&#39; option and select &#39;Open in new tab&#39;. This should open the Jupyter web interface and all of the material for the Fastai course. . . 4 - When you open your first notebook, you&#39;ll be asked to select a kernel. Make sure to choose the &#39;fastai&#39; kernel. If that isn&#39;t an option, give it 20 minutes as your instance will need a bit of time to setup the first time it is opened. If it still doesn&#39;t appear as a kernel option then you might need to use the following YAML file that a very kind person on that forums has created (I had to do this and then repeat step 2 in the 3.3 &#39;Stack Setup&#39; section): . https://forums.fast.ai/t/sagemaker-notebook-deployment-problem-no-fastai-kernel/88806/10?u=chelsea . . 4.2 Important . Important! Always stop your notebook instance when you are done. If you don&#39;t stop your notebook instance from running, you will continue to be charged. Always make sure to stop all of your notebook instances before you sign out of your AWS account. . .",
            "url": "https://mathschelsea.github.io/Blogging/jupyter/2021/07/12/setup.html",
            "relUrl": "/jupyter/2021/07/12/setup.html",
            "date": " • Jul 12, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "And so it begins...",
            "content": "Mission Statement . &quot;I will not rest until I have you holding a Coke, wearing your own shoe, playing a Sega game featuring you, while singing your own song in a new commercial, starring you, broadcast during the Superbowl, in a game that you are winning, and I will not sleep until that happens&quot; - Jerry Maguire . These writings don&#39;t aim to impress. They are simply an account of one person&#39;s learning curve. The intention is to showcase an honest journey that is accessible to all. Too often topics like Data Science are shrouded in mystery, kept from the masses by large organisations and the highest levels of education. When learnings are shared, they usually alienate eager pupils with overly-complicated content. This suffocation of aspiration needs to end and a new wave of alternative learning needs to emerge. It&#39;s time to be kind and share our journey - the good and the bad. It&#39;s also time to be gracious and remember that all teachers were once pupils. Here is my account. . Who is Chelsea Tucker? . A lover of all things technical, I spend my day as a Data Scientist building machine learning models that; in a nutshell, predict the future. Pretty neat. By evening I champion and promote women in STEM subjects through speaking at events and creating content on Instagram, YouTube and TikTok. I aim to ignite a belief in young people that they have what it takes to thrive in a world of numbers and computing; regardless of gender or race. I also acknowledge that not everyone is dealt the same privileges in life and so I create free visually appealing mathematical learning materials using my background in both fine art and mathematics. And I do all of this with copious amounts of Yorkshire tea and intermittent holidays that revolve around longboarding, kitesurfing and general &quot;vanlife&quot; antics. . .",
            "url": "https://mathschelsea.github.io/Blogging/jupyter/2021/07/02/begin.html",
            "relUrl": "/jupyter/2021/07/02/begin.html",
            "date": " • Jul 2, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": ". Chelsea Tucker . A lover of all things technical, I spend my day as a Data Scientist building machine learning models that; in a nutshell, predict the future. Pretty neat. By evening I champion and promote women in STEM subjects through speaking at events and creating content on Instagram, YouTube and TikTok. I aim to ignite a belief in young people that they have what it takes to thrive in a world of numbers and computing; regardless of gender or race. I also acknowledge that not everyone is dealt the same privileges in life and so I create free visually appealing mathematical learning materials using my background in both fine art and mathematics. And I do all of this drinking copious amounts of Yorkshire tea, buying aesthetically pleasing stationary and planning holidays that revolve around longboarding, kitesurfing and general “vanlife” antics. . Blog Mission Statement - Jerry Maguire style . These writings don’t aim to impress. They are simply an account of one person’s learning curve. The intention is to showcase an honest journey that is accessible to all. Too often topics like Data Science are shrouded in mystery, kept from the masses by large organisations and the highest levels of education. When learnings are shared, they usually alienate eager pupils with overly-complicated content. This suffocation of aspiration needs to end and a new wave of alternative learning needs to emerge. It’s time to be kind and share our journey - the good and the bad. It’s also time to be gracious and remember that all teachers were once pupils. . MathsChelsea . The intention behind the ‘MathChelsea’ Instagram and Youtube account was to create a platform of freely accessible mathematical learning materials that rebel against the notion that maths is difficult, mundane and irrelevant. The result is a collection of worksheets and videos that teach technical topics through bursts of colour and unconventional contexts. In addition to learning materials, these socials also house illustrations that champion female figures in STEM who come from underprivileged or minority backgrounds. . . Bits and Bobs . Education: . MMath Mathematics at The University of Leeds | Msc Exploration Geophysics at The University of Leeds | . STEM: . Stemettes 2021 #EasterExplore Speaker | YFYA 2020 Speaker | Stemettes 2020 #Outbox2020 Speaker | Women in Mathematics Article | Tips for Maths Careers Article | .",
          "url": "https://mathschelsea.github.io/Blogging/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mathschelsea.github.io/Blogging/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}